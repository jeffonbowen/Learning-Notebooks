---
title: "Jeff's R Reference Notes"
output: 
  html_notebook:
    toc: true
    toc_float: true
  github_document: default
  
---
# General Reference and Learning Materials 

Here are some of the reference and learning materials that I have found useful.    

* R for Data Science. <https://r4ds.had.co.nz/transform.html>. I think this is a good entry to R after having learned just the basics of base R.  
* Data Camp seems very good for guided training.  
* Cookbook for R is handy, though wish it used tidy functions more.  
* Also the companion ggplot2 cookbook, <http://www.cookbook-r.com/Graphs/>
* This UBC course site has good references for fitting models in ecology.  <https://www.zoology.ubc.ca/~schluter/R/Model.html>  
* Ecology in R (see Udemy website).   
* This is a long list of reference materials: <https://paulvanderlaken.com/2017/08/10/r-resources-cheatsheets-tutorials-books/>
* This is a good reference for the most-common aspects of R Markdown. <https://www.r-bloggers.com/getting-started-with-r-markdown-guide-and-cheatsheet/>
* Good blog post on good style. <https://kdestasio.github.io/post/r_best_practices/>
* Some good stuff about working in the R environment <https://rstats.wtf/>
* Jenny Bryan course notes book <https://stat545.com/>
* <https//:guru99.com> has some good tutorials. 
* This is a good reference list. <https://github.com/iamericfletcher/r-learning-resources/blob/main/README.md>



***

# Update R
From R (not in RStudio) use package 'installr' and run `updateR()`.

***

# Import/Read Data

Best to use relative paths and not relay on setting working directory. But this is how you do it. 
```
getwd()  
setwd("c:/      ")  
list.files()      Useful for copying a filename '  
```

Package readr for importing csv files and setting column types.
```
indat <- read_csv('CEM_Oct10_13.csv', 
                  col_types = cols(.default = "c",
                                   'Shape_Area' = col_double(),
                                   'SDEC_1' = col_integer(), 
                                   'SDEC_2' = col_integer(), 
                                   'SDEC_3' = col_integer()))  
```
Package readxl to read MS Excel files.
`stations <- read_excel("dat/Songto2019data.xlsx", "Stations")`

## Setting Default for all columns  
`col_types = cols(.default = "c")`

Interactive file selection:  
`file.choose()`

***

# View and clean data

`Dim()` gives dimensions, but that is in the environment window anyway.  
`Print()`  
`head()` is handy for initial look at data.   
`tail()`  
`str()`  
`summary()`  
`merge()` from base R. I would use dplyr. 
`rm()` remove from memory

To check for missing values and nulls, use `is.na()` and `is.null()`.

to get of rows with NA, use `na.omit()`.

To get list of unique values, use `distinct()`or `count()` which provides frequency. Its preferable to using the base `table()` function. `janitor::tabyl()` is another option for a 2-way frquecny table.

To find duplicates, can use:
```
mtcars %>% 
  group_by(carb) %>% 
  filter(n()>1)
```
Or `janitor::get_dupes()`

To check coordinates, use `mapview()`.

---
# Data Manipulation 

library(tidyverse)
library(dplyr)    
library(lubridate), to work wth time.

Rename columns
colnames(x) <- c("a", "b")    Base R
rename(x, newname = oldname)  Tidy way

Subsetting data
tmp <- dat[dat$SERAL_1 == "AS" & !is.na(dat$SERAL_1), ] second part deals with NA rows

To sort, use
`arrange()`
Create variables
`mutate()`  Benefit over base: more efficient with more than one variable.

Filter out NA values
x <- x %>% filter(!is.na(BHC20_2))

Recode values using base R
dat[dat$SERAL_1 == "AS" & !is.na(dat$SERAL_1), ]$SERAL_1 <- "as"

The tidy way is to uese `if_else()` or if many substittions, `case_when()`. In the example below, note the use of `TRUE ~ variable` for all other values to stay the same.
```
country_totals <- country_totals %>% 
  mutate(country_name = 
           case_when(
             country == "UK" ~ "United Kingdom",
             TRUE ~ country_name
           )
  )
```

To  convert to factor
StationIDxEnvTest$CountOfStationID <- as.factor(StationIDxEnvTest$CountOfStationID)

## Abundance to Presence-Absence
Can use `if_else(across(...), ...)` or something like that.

# Count and tally

Group_by and Summarize
sumdat <- 
  data %>% 
  group_by(GRID_ID, PRVUD, PRVUDOF, Accessible, Ownership, Candidate2, Region, BHC20_1) %>% 
  summarise(Total = sum(Shape_Area/10000)) %>% 
  top_n(1, Total) %>% 
  filter(Candidate2 == "Yes")


---
# Display and Formatting

For turning strings in to numbers, use 'stringr'.
See examples in Home Finances project.

---
# Exploratory Data Analysis

The TidyTuesday Penguin data analysis has some examples.
https://github.com/jeffonbowen/TidyTuesday

---
# Visualization 
library(ggplot2)              For graphics
library(cowplot)              addon to ggplot
library(ggrepel)              Add on for moving labels around

g2 <- ggplot(emmE.plot, aes(x=xEasting, y=mean, group=Habitat))+
  geom_line(aes(colour=Habitat))+    
  scale_size_manual(10)+
  scale_colour_manual(values=c("#1D4D18", "#5BC355", "#ABEB22","#EBB57C"))+
  xlab(label = "<<< West          -Location-          East >>>") +
  ylab(label = "Estimated mean density (males/ha) with 95% CI")+
  labs(title=spp)+
  theme_bw()+
  geom_ribbon(aes(ymin = LCL, ymax = UCL, fill=Habitat, colour=NA), alpha=0.1, linetype=0)
g2

---
# Modelling

## Reference
There is some good material here on models and model viz. <https://uvastatlab.github.io/phdplus/index.html>

Reminder about descriptive & explanatory models versus predictive models. Descriptive and explanatory model fit sually evaluted using AIC or similar. Predictive models tested with prediction accuracy (e.g., AUC).  



## Packages for assessing data distribution
First use simple frequency histograms using `hist()` of ggplot.
library(vcd)                  Fits a discrete (count data) distribution for goodness-of-fit tests.

## Packages for regression-type analyses
Common packages for regression-type analyses
library(lme4)                 GLMMs
library(MASS)                 Negative Binomial GLM
library(glmmTMB)              for nb and zi, but can do everyhting esle too
library(pscl)                 zero-inflated models
library(mgcv)                 GAM

## Packages for evaluating model fit
library(DHARMa)               Analysis of residuals for mixed models

## Common packages for interpretting results
library(visreg)               regression visualization
library(MuMIn)                model selection
library(effects)              for extracting effects terms. Use effects or Alleffects
library(emmeans)              Estimate marginal means

## Helpers
library(broom)                Tidy results tables for exporting and printing 
library(broom.mixed)          Same as above, but for mixed models

library(corrplot)             Come back to this and find home

## Typical steps for regression

**STEP 1. Visualize distribution**
Make a histogram using hist() or ggplot(); visually inspect
For count data, use package 'vcd' to fit possible disubutions
fit.p <- goodfit(dat$Richness, type = "poisson")  also binomial and nbinomial
summary(fit.p)
rootogram(fit.p)

STEP 2. Fit model

Use `dredge(model.full)` to fit all possible combinations of variables.


STEP 3. Compare models
Comare models using package 'MuMin'. 
model.sel(m1, m2, m3, m4)
** Figure out weights and model averaging

STEP 4. Assess model fit. 
If glm, view standard diagnostic plots
`plot(model1)`  or
Examine Residuals for Gaussian
`qqnorm(residuals(bestmod))`
`qqline(residuals(bestmod))`
`plot(bestmod,MonYear~resid(.))`

Visreg?

For mixed models, use package 'DHARMa' to simulate residuals.
res <- simulateResiduals(fittedModel = bestmod)
plot(res)
testDispersion(res)
There are other test in 'DHAMRa' to explore.
Test for overdispersion. Unlear of these work for mixed models. 
```{r eval=FALSE}
deviance(bestmod)/df.residual(bestmod)
# or
overdisp_fun <- function(model) {
  rdf <- df.residual(model)
  rp <- residuals(model,type="pearson")
  Pearson.chisq <- sum(rp^2)
  prat <- Pearson.chisq/rdf
  pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)
  c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)
}
overdisp_fun(bestmod)
```
**STEP 5. Estimate Means and Plot** 
Plot all effects
ae <- allEffects(bestmod)
plot(ae, residuals="TRUE")
ae
Or plot individual effects
e <- predictorEffect("BHC7", bestmod)
plot(e)

Package EMMEANS for marginal means

To make predictions, may need to create a grid.
expand.grip, tidyr:: expand_grid, modelr::data_grid

What a out predictions for continuous variables?
emmE <- as.data.frame(emmeans(pois.m2, specs="xEasting", 
                              at=list(xEasting=seq(-2.03, 1.13, length.out=50))))



### Extract the coefficients data frame
x <-summary.glm(Count.model1)$coefficients 
---
# Generalized Additive Models
See Gavin Simpson seminar on GAMs. 
https://github.com/gavinsimpson/intro-gam-webinar-2020

* A GAM is a sum of smooth functions (basis functions or small curves).
* Start with using REML as default. Most robust to violation of assumptions.
* Categorical variables - can't smooth, but can be included in the model. Can be used as a random effect as well. 
* Random effects can be fit in gam() and bam() if simple, without having to use gamm() of gamm4(). bs = 're'. See slids for more explanation of options.

Next steps from Gavin Simpson:

* Read Simon Wood's book!

* Lots more material on our ESA GAM Workshop site
  [https://noamross.github.io/mgcv-esa-workshop/]()

* Noam Ross' free GAM Course
  <https://noamross.github.io/gams-in-r-course/>

A couple of papers:

1. Simpson, G.L., 2018. Modelling Palaeoecological Time Series Using Generalised Additive Models. Frontiers in Ecology and Evolution 6, 149. https://doi.org/10.3389/fevo.2018.00149

2. Pedersen, E.J., Miller, D.L., Simpson, G.L., Ross, N., 2019. Hierarchical generalized additive models in ecology: an introduction with mgcv. PeerJ 7, e6876. https://doi.org/10.7717/peerj.6876
]



---
# Classification
library(rpart) for Recursive Classification and Regression. To produce a classification tree. 

For plotting, package rpart.plot.

Example before fro TidyTuesday Penguins. https://github.com/jeffonbowen/TidyTuesday

```{r}
fitr <- rpart(data = penguins, formula = species ~ bill_length_mm + bill_depth_mm, 
              flipper_length_mm + body_mass_g)
fitr
summary(fitr)
# Print table of optimal prunings
printcp(fitr)
# Plot
plot(fitr)
text(fitr)
```


---
# Biodiversity Analyses

library(vegan)
library(dismo)                Species distribution models
library(BiodiversityR)
library(indicspecies)         Indicator species analysis


RankAbun.1 <- rankabuncomp(stxsp, y=st.year, factor='Survey.Name', scale='abundance', 
                           legend=F, ylim = c(0,600), rainbow=T)
RankAbun.1 <- rankabuncomp(stxsp, y=st.year, factor='Survey.Name', scale='proportion',
                           legend=T, rainbow=T)
RankAbun.1 <- rankabuncomp(stXspAbunSMSA, y=stXyearSMSA, factor='Survey.Name', scale='proportion',
                           legend=T, rainbow=T)
```
library(iNEXT)
out <- iNEXT(spider, q=c(0,1,2), datatype="abundance", size=m)
ggiNEXT(x, type=1, se=TRUE, facet.var="order", color.var="site", grey=FALSE)

DataInfo(spXyearSMSA)
out <- iNEXT(spXyearSMSAn, q=0, datatype="incidence_freq",endpoint=336)
out <- iNEXT(spXyearSMSAn, q=c(0,1,2), datatype="incidence_freq",endpoint=336)
ggiNEXT(out,type=1,se=TRUE)
ggiNEXT(out,type=2,se=TRUE)
ggiNEXT(out, type=1, facet.var="order")

library(SpadeR)
SimilarityMult(spxyear.1i, datatype = "incidence_freq", q = 0, nboot = 200)
```
---

# Dissimilarity
Two key functions:
vegdist(), method - "hellinger"
decostand()


# Markdown

kable() for presentation of tables.

---
# Ordination
Putting things in order.
* Arrange samples samples along gradients
* Map data to lower dimensions

## Unconstrained
First we look for major variation, then relate to environmental variation. 

PCA: Linear model. Most useful for env data, sometimes species data.
CA: Unimodal method. Species data and presence-absence, esp. where non-linear responses are observed. Very similar to PCA, ecept weighted form. 
PCO and NMDS: Any kind of data. Doesn't care about gradinet, just dissimilarity. 

## NMDS from using Palmer Penguins data. 

```{r}
pengv <- penguins %>% 
  select(3:6)

nmds <- metaMDS(pengv, distance = "bray")

# extract NMDS scores (x and y coordinates)
data.scores = as.data.frame(scores(nmds))

# add columns to data frame 
data.scores$species <- penguins$species
data.scores$sex <- penguins$sex

n_plot <- ggplot(data.scores, aes(x = NMDS1, y = NMDS2)) + 
  geom_point(size = 2, aes(shape = sex, colour = species))+ 
  theme(axis.text.y = element_text(colour = "black", size = 10, face = "bold"), 
        axis.text.x = element_text(colour = "black", face = "bold", size = 10), 
        legend.text = element_text(size = 10, face ="bold", colour ="black"), 
        legend.position = "right", axis.title.y = element_text(face = "bold", size = 11), 
        axis.title.x = element_text(face = "bold", size = 10, colour = "black"), 
        legend.title = element_text(size = 11, colour = "black", face = "bold"), 
        panel.background = element_blank(), panel.border = element_rect(colour = "black", fill = NA, size = 1.2),
        legend.key=element_blank()) + 
  labs(x = "NMDS1", colour = "Species", y = "NMDS2", shape = "Sex") 
n_plot

ggsave(plot = n_plot, "nmds_plot.png")
```

---
# Mapping

## Main packages for mapping in R
library(sp)         #Key package. Though being replaced by sf.
library(sf)         #Replaces sp. sp still needed for some things.
library(rgdal)      #Bindings for GDAL(?) seems to provide helper functions
library(raster)     #This is key for raster manipulation.
library(rasterVis)
library(maptools)   #Haven't used this yet

library(rgeos)      ? Has some data. Geometry Engine
library(tmap)
library(tmaptools)
library(ggmap)      #Spatial Visualization with ggplot2

library(rspatial)

library(maps)

library(bcmaps)

## For dynamic maps

library(leaflet)    Key package for dynamic maps. 
library(mapview)    Wrapper for leaflet. Great for quick viewing.

## Web integration 
library(htmlwidgets)
library(shiny)
library(webshot)

## Static Maps
{gglot} keeps thingsin the tidyverse.

{rnaturalearth} is good source for data and works nice with sf and ggplot.

```{r}
world <- ne_countries(scale = "medium", returnclass = "sf")
world %>% 
  ggplot() +
  geom_sf() +
  ggtitle("World")
```

Mapview, good for quick view
`mapview(x)`

Leaflet
```
m <- leaflet() %>% 
  addTiles() 
```

Lat/long NAD83 is EPSG 4269
Lat/long WGS84 is EPSG 4326
UTM NAD83 zone 10N is EPSG 26911 

How to reproject
```
newcrs = CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")
footprint <- spTransform(footprint, newcrs)
```

Read/Write shapefile
library(rgdal)
writeOGR(footprint, dsn = ".", "fp_wgs84", driver="ESRI Shapefile")

Read UTM cooridnates and write same datafile with lat/long 
dat <- read_csv("WOODto2019.csv") 
dat <- SpatialPointsDataFrame(coords = cbind(woodat$UTM_Northing, woodat$UTM_Easting), 
                              data = woodat, proj4string=CRS("+proj=utm +zone=10 +north +datum=NAD83"))  
dat <- spTransform(wooutm, CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
datout <- (cbind(dat@data, dat@coords)) %>% 
                  rename(Longitude = coords.x1, Latitude = coords.x2)

Rasters
GDALinfo("") raster file attributes before reading. 
