---
title: "R Reference Notes"
output: 
  html_notebook:
    toc: true
    toc_float: true
  github_document: default
  
---  

**Note:** Some topics are now contained in their own file.   


# General Reference and Learning Materials 

Here are some of the reference and learning materials that I have found useful.      
* R for Data Science. <https://r4ds.had.co.nz/transform.html>. I think this is a good entry to R after having learned just the basics of base R.  
* Data Camp seems very good for guided training.  
* Cookbook for R is handy, though wish it used tidy functions more.  
* Also the companion ggplot2 cookbook, <http://www.cookbook-r.com/Graphs/>
* This UBC course site has good references for fitting models in ecology.  <https://www.zoology.ubc.ca/~schluter/R/Model.html>  
* Ecology in R (see Udemy website).   
* This is a long list of reference materials: <https://paulvanderlaken.com/2017/08/10/r-resources-cheatsheets-tutorials-books/>
* This is a good reference for the most-common aspects of R Markdown. <https://www.r-bloggers.com/getting-started-with-r-markdown-guide-and-cheatsheet/>
* Good blog post on good style. <https://kdestasio.github.io/post/r_best_practices/>
* Some good stuff about working in the R environment <https://rstats.wtf/>
* Jenny Bryan course notes book <https://stat545.com/>
* <https//:guru99.com> has some good tutorials. 
* This is a good reference list. <https://github.com/iamericfletcher/r-learning-resources/blob/main/README.md>



***

# Update R
From R (not in RStudio) use package 'installr' and run `updateR()`.

***

# Import/Read Data

Best to use relative paths and not relay on setting working directory. But this is how you do it. 
```
getwd()  
setwd("c:/      ")  
list.files()      Useful for copying a filename '  
```
Package readr for importing csv files and setting column types.
```
indat <- read_csv('CEM_Oct10_13.csv', 
                  col_types = cols(.default = "c",
                                   'Shape_Area' = col_double())
```
Package readxl to read MS Excel files.
`stations <- read_excel("dat/Songto2019data.xlsx", "Stations", sheet = xxx, skip = lines)`

## Setting Default for all columns  
`col_types = cols(.default = "c")`

Interactive file selection:  
`file.choose()`

***

# View and clean data

`Dim()` gives dimensions, but that is in the environment window anyway.  
`head()` is handy for initial look at data.   
`tail()`  
`str()`  
`summary()`  
`rm()` remove from memory

`glimpse()` is a nice way to have a quick peak at data. Can use at end of pipe. 

To check for missing values and nulls, use `is.na()` and `is.null()`.
To get rid of rows with NA, use `na.omit()`.

To get list of unique values, use `n_distinct()`or `count()` which provides frequency. Its preferable to using the base `table()` function.  

Package {janitor} helpful for cleaning data. 
`janitor::clean_names()`
`janitor::tabyl()` is another option for a 2-way frequency table.

To find duplicates, can use:
```
mtcars %>% 
  group_by(carb) %>% 
  filter(n()>1)
```
Or `janitor::get_dupes()`

To check coordinates, use `mapview()`.

---
# Data Manipulation 

library(tidyverse)
library(dplyr)    
library(lubridate), to work wth time.

Rename columns
colnames(x) <- c("a", "b")    Base R
rename(x, newname = oldname)  Tidy way

Subsetting data
tmp <- dat[dat$SERAL_1 == "AS" & !is.na(dat$SERAL_1), ] second part deals with NA rows

To sort, use
`arrange()`
Create variables
`mutate()`  Benefit over base: more efficient with more than one variable.

Filter out NA values
x <- x %>% filter(!is.na(BHC20_2))

Recode values using base R
dat[dat$SERAL_1 == "AS" & !is.na(dat$SERAL_1), ]$SERAL_1 <- "as"

The tidy way is to uese `if_else()` or if many substittions, `case_when()`. In the example below, note the use of `TRUE ~ variable` for all other values to stay the same.
```
country_totals <- country_totals %>% 
  mutate(country_name = 
           case_when(
             country == "UK" ~ "United Kingdom",
             TRUE ~ country_name
           )
  )
```

To  convert to factor
StationIDxEnvTest$CountOfStationID <- as.factor(StationIDxEnvTest$CountOfStationID)

## Abundance to Presence-Absence
Can use `if_else(across(...), ...)` or something like that.

# Count and tally

Group_by and Summarize
sumdat <- 
  data %>% 
  group_by(GRID_ID, PRVUD, PRVUDOF, Accessible, Ownership, Candidate2, Region, BHC20_1) %>% 
  summarise(Total = sum(Shape_Area/10000)) %>% 
  top_n(1, Total) %>% 
  filter(Candidate2 == "Yes")


---
# Display and Formatting

For turning strings in to numbers, use 'stringr'.
See examples in Home Finances project.

---
# Exploratory Data Analysis

There are a couple of packages that can help automate initial exploration.  
{skimr}
{DataExplorer}  

The TidyTuesday Penguin data analysis has some of my own examples.
https://github.com/jeffonbowen/TidyTuesday



---
# Visualization 
library(ggplot2)              For graphics
library(cowplot)              addon to ggplot
library(ggrepel)              Add on for moving labels around

g2 <- ggplot(emmE.plot, aes(x=xEasting, y=mean, group=Habitat))+
  geom_line(aes(colour=Habitat))+    
  scale_size_manual(10)+
  scale_colour_manual(values=c("#1D4D18", "#5BC355", "#ABEB22","#EBB57C"))+
  xlab(label = "<<< West          -Location-          East >>>") +
  ylab(label = "Estimated mean density (males/ha) with 95% CI")+
  labs(title=spp)+
  theme_bw()+
  geom_ribbon(aes(ymin = LCL, ymax = UCL, fill=Habitat, colour=NA), alpha=0.1, linetype=0)
g2

---

# Regression Models

***Moved to its own file.***  

---
# Classification
library(rpart) for Recursive Classification and Regression. To produce a classification tree. 

For plotting, package rpart.plot.

Example before fro TidyTuesday Penguins. https://github.com/jeffonbowen/TidyTuesday

```{r}
fitr <- rpart(data = penguins, formula = species ~ bill_length_mm + bill_depth_mm, 
              flipper_length_mm + body_mass_g)
fitr
summary(fitr)
# Print table of optimal prunings
printcp(fitr)
# Plot
plot(fitr)
text(fitr)
```


---
# Biodiversity Analyses

library(vegan)
library(dismo)                Species distribution models
library(BiodiversityR)
library(indicspecies)         Indicator species analysis


RankAbun.1 <- rankabuncomp(stxsp, y=st.year, factor='Survey.Name', scale='abundance', 
                           legend=F, ylim = c(0,600), rainbow=T)
RankAbun.1 <- rankabuncomp(stxsp, y=st.year, factor='Survey.Name', scale='proportion',
                           legend=T, rainbow=T)
RankAbun.1 <- rankabuncomp(stXspAbunSMSA, y=stXyearSMSA, factor='Survey.Name', scale='proportion',
                           legend=T, rainbow=T)
```
library(iNEXT)
out <- iNEXT(spider, q=c(0,1,2), datatype="abundance", size=m)
ggiNEXT(x, type=1, se=TRUE, facet.var="order", color.var="site", grey=FALSE)

DataInfo(spXyearSMSA)
out <- iNEXT(spXyearSMSAn, q=0, datatype="incidence_freq",endpoint=336)
out <- iNEXT(spXyearSMSAn, q=c(0,1,2), datatype="incidence_freq",endpoint=336)
ggiNEXT(out,type=1,se=TRUE)
ggiNEXT(out,type=2,se=TRUE)
ggiNEXT(out, type=1, facet.var="order")

library(SpadeR)
SimilarityMult(spxyear.1i, datatype = "incidence_freq", q = 0, nboot = 200)
```
---

# Dissimilarity
Two key functions:
vegdist(), method - "hellinger"
decostand()


# Markdown

kable() for presentation of tables.

---
# Ordination
Putting things in order.
* Arrange samples samples along gradients
* Map data to lower dimensions

## Unconstrained
First we look for major variation, then relate to environmental variation. 

PCA: Linear model. Most useful for env data, sometimes species data.
CA: Unimodal method. Species data and presence-absence, esp. where non-linear responses are observed. Very similar to PCA, ecept weighted form. 
PCO and NMDS: Any kind of data. Doesn't care about gradinet, just dissimilarity. 

## NMDS from using Palmer Penguins data. 

```{r}
pengv <- penguins %>% 
  select(3:6)

nmds <- metaMDS(pengv, distance = "bray")

# extract NMDS scores (x and y coordinates)
data.scores = as.data.frame(scores(nmds))

# add columns to data frame 
data.scores$species <- penguins$species
data.scores$sex <- penguins$sex

n_plot <- ggplot(data.scores, aes(x = NMDS1, y = NMDS2)) + 
  geom_point(size = 2, aes(shape = sex, colour = species))+ 
  theme(axis.text.y = element_text(colour = "black", size = 10, face = "bold"), 
        axis.text.x = element_text(colour = "black", face = "bold", size = 10), 
        legend.text = element_text(size = 10, face ="bold", colour ="black"), 
        legend.position = "right", axis.title.y = element_text(face = "bold", size = 11), 
        axis.title.x = element_text(face = "bold", size = 10, colour = "black"), 
        legend.title = element_text(size = 11, colour = "black", face = "bold"), 
        panel.background = element_blank(), panel.border = element_rect(colour = "black", fill = NA, size = 1.2),
        legend.key=element_blank()) + 
  labs(x = "NMDS1", colour = "Species", y = "NMDS2", shape = "Sex") 
n_plot

ggsave(plot = n_plot, "nmds_plot.png")
```

---
# Mapping

## Main packages for mapping in R
library(sp)         #Key package. Though being replaced by sf.
library(sf)         #Replaces sp. sp still needed for some things.
library(rgdal)      #Bindings for GDAL(?) seems to provide helper functions
library(raster)     #This is key for raster manipulation.
library(rasterVis)
library(maptools)   #Haven't used this yet

library(rgeos)      ? Has some data. Geometry Engine
library(tmap)
library(tmaptools)
library(ggmap)      #Spatial Visualization with ggplot2

library(rspatial)

library(maps)

library(bcmaps)

## For dynamic maps

library(leaflet)    Key package for dynamic maps. 
library(mapview)    Wrapper for leaflet. Great for quick viewing.

## Web integration 
library(htmlwidgets)
library(shiny)
library(webshot)

## Static Maps
{gglot} keeps thingsin the tidyverse.

{rnaturalearth} is good source for data and works nice with sf and ggplot.

```{r}
world <- ne_countries(scale = "medium", returnclass = "sf")
world %>% 
  ggplot() +
  geom_sf() +
  ggtitle("World")
```

Mapview, good for quick view
`mapview(x)`

Leaflet
```
m <- leaflet() %>% 
  addTiles() 
```

Lat/long NAD83 is EPSG 4269
Lat/long WGS84 is EPSG 4326
UTM NAD83 zone 10N is EPSG 26911 

How to reproject
```
newcrs = CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")
footprint <- spTransform(footprint, newcrs)
```

Read/Write shapefile
library(rgdal)
writeOGR(footprint, dsn = ".", "fp_wgs84", driver="ESRI Shapefile")

Read UTM cooridnates and write same datafile with lat/long 
dat <- read_csv("WOODto2019.csv") 
dat <- SpatialPointsDataFrame(coords = cbind(woodat$UTM_Northing, woodat$UTM_Easting), 
                              data = woodat, proj4string=CRS("+proj=utm +zone=10 +north +datum=NAD83"))  
dat <- spTransform(wooutm, CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
datout <- (cbind(dat@data, dat@coords)) %>% 
                  rename(Longitude = coords.x1, Latitude = coords.x2)

Rasters
GDALinfo("") raster file attributes before reading. 
